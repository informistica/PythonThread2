<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title>Documento senza titolo</title>
</head>

<body>
<div>
  <div> <a href="https://blog-floydhub-com.translate.goog/tag/data-science/?_x_tr_sl=auto&amp;_x_tr_tl=it&amp;_x_tr_hl=it">Scienza dei dati</a>
    <h1>Multiprocessing vs. Threading in Python: ciò che ogni scienziato di dati deve sapere</h1>
    <p>Questo  approfondito approfondimento sulle librerie di parallelizzazione Python  - multiprocessing e threading - spiegherà quali utilizzare quando per  diversi set di problemi di data scientist.</p>
    <div>
      <div>
        <h4> </h4>
      </div>
    </div>
    <p>Prima o poi, ogni progetto di data science deve affrontare una sfida inevitabile: la velocità. Lavorare  con set di dati più grandi porta a un'elaborazione più lenta degli  stessi, quindi alla fine dovrei pensare di ottimizzare il tempo di  esecuzione del tuo algoritmo. Come molti di voi già sanno, la parallelizzazione è un passaggio necessario di questa ottimizzazione. Python offre due librerie integrate per la parallelizzazione: multiprocessing e threading. In  questo articolo, esploreremo come i data scientist possono scegliere  tra i due e quali fattori dovrebbero essere tenuti presenti mentre lo  fanno.</p>
    <h2 id="parallelcomputinganddatascience">Calcolo parallelo e scienza dei dati</h2>
    <p>Come  tutti sapete, la scienza dei dati è la scienza che consente di gestire  grandi quantità di dati e di estrarne informazioni utili. Il  più delle volte, le operazioni che eseguiamo sui dati sono facilmente  parallelizzabili, il che significa che diversi agenti di possono  eseguire l'operazione sui dati un pezzo alla volta, quindi combinare i  risultati alla fine per ottenere il risultato completo.</p>
    <p>Per visualizzare meglio la parallelizzabilità, consideriamo un'analogia con il mondo reale. Supponiamo di dover pulire tre stanze della tua casa. Puoi  fare tutto da solo, pulendo le stanze una dopo l'altra, oppure puoi  chiedere ai tuoi due fratelli di aiutare, ognuno di voi pulisce una  singola stanza. In  quest'ultimo approccio, ognuno di voi sta lavorando parallelamente su  una parte dell'intero compito, riducendo così il tempo totale necessario  per completarlo. Questa è la parallelizzabilità in azione.</p>
    <p>L'elaborazione parallela può essere ottenuta in Python in due modi diversi: multiprocessing e threading.</p>
    <h2 id="multiprocessingandthreadingtheory">Multiprocessing e Threading: Teoria</h2>
    <p>Fondamentalmente,  il multiprocessing e il threading sono due modi per ottenere il calcolo  parallelo, utilizzando rispettivamente processi e thread come agenti di  elaborazione. Per capire come lavorare, dobbiamo chiarire cosa sono i processi.</p>
    <p><img src="https://www.elexpo.net/archivio/tps/parallelismo/Multiprocessing%20vs.%20Threading%20in%20Python%20What%20Every%20Data%20Scientist%20Needs%20to%20Know_files/apps-processes-threads.png" alt="app-processi-thread" loading="lazy" width="895" height="498" /></p>
    <h3 id="process">Processi</h3>
    <p>Un processo è un'istanza di un programma per computer in esecuzione. Ogni  processo ha il proprio spazio di memoria che utilizza per archiviare le  istruzioni in esecuzione, nonché tutti i dati necessari per archiviare e  accedere per l'esecuzione.</p>
    <h3>Filo</h3>
    <p>I thread sono componenti di un processo che possono essere eseguiti in parallelo. Possono  esserci più thread in un processo e condividono lo stesso spazio di  memoria, ovvero lo spazio di memoria del processo padre. <br />
      Ciò significherebbe il codice da eseguire e tutte dichiarate nel programma condiviso da tutti i thread. </p>
    <p><img src="https://www.elexpo.net/archivio/tps/parallelismo/Multiprocessing vs. Threading in Python What Every Data Scientist Needs to Know_files/processi-e-thread.png" alt="" width="392" height="333" /></p>
    <p>Processo e thread, di I, <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://commons.wikimedia.org/wiki/User:Cburnett" title="Utente: Cburnett">Cburnett</a> , <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=http://creativecommons.org/licenses/by-sa/3.0/" title="Attribuzione Creative Commons-Condividi allo stesso modo 3.0">CC BY-SA 3.0</a> , <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://commons.wikimedia.org/w/index.php?curid%3D2233446">collegamento</a></p>
    <p>Ad esempio, consideriamo i programmi nell'esecuzione sul tuo computer in questo momento. Probabilmente stai leggendo questo articolo in un browser, che probabilmente ha più schede aperte. anche ascoltare musica tramite l'app desktop Spotify allo stesso tempo. Il browser e l'applicazione Spotify sono processi diversi; ognuno di essi può utilizzare più processi o thread per ottenere il parallelismo. Schede diverse nel tuo browser potrebbero essere eseguite in thread diversi. Spotify  può riprodurre musica in un thread, scaricare musica da Internet in un  altro e utilizzarne un terzo per visualizzare la GUI. Questo sarebbe chiamato multithreading. Lo stesso può essere fatto con il multiprocessing, anche con più processi.In  effetti, la maggior parte dei browser moderni come Chrome e Firefox  utilizza il multiprocessing, non il multithreading, per gestire più  schede.</p>
    <h3 id="technicaldetails">Dettagli tecnici</h3>
    <ul>
      <li>
        <p>Tutti i thread di un processo vivono nello stesso spazio di memoria, mentre i processi hanno il loro spazio di memoria separato.</p>
      </li>
      <li>
        <p>I thread sono più leggeri e hanno un sovraccarico inferiore rispetto ai processi. I processi di spawn sono un po' più lenti dei thread di spawn.</p>
      </li>
      <li>
        <p>La condivisione di oggetti tra thread è più semplice, poiché lo stesso spazio di memoria. Per  ottenere lo stesso risultato tra i processi, dobbiamo utilizzare una  sorta di modello IPC (comunicazione tra processi), generalmente fornito  dal sistema operativo.</p>
      </li>
    </ul>
    <h3 id="pitfallsofparallelcomputing">Insidie ​​del calcolo parallelo</h3>
    <p>L'introduzione del parallelismo in un programma non è sempre un gioco a somma positiva; ci sono alcune insidie ​​di cui essere consapevoli. I più importanti sono i seguenti.</p>
    <ul>
      <li><strong>Condizione di competizione</strong> : come abbiamo già discusso, i thread hanno uno spazio di memoria  condivisa e quindi possono avere accesso a variabili condivisi. Una race condition si quando più thread tentano di modificare la stessa variabile contemporaneamente. Lo  scheduler dei thread può scambiare arbitrariamente tra i thread, quindi  non abbiamo modo di conoscere l'ordine in cui i thread cercheranno di  modificare i dati. Ciò può  comportare un comportamento errato in uno dei thread, in particolare se  i thread decidono di eseguire qualcosa in base al valore della  variabile. Per evitare che ciò accada, è possibile posizionare un <em>blocco</em> di esclusione reciproca (o mutex) attorno alla parte di codice che  modifica la variabile in modo che un solo thread alla volta possa  scrivere sulla variabile.</li>
      <li><strong>Starvation</strong> : la fame si verifica quando a un thread viene negato l'accesso a una  determinata risorsa per periodi di tempo più lunghi e, di conseguenza,  il programma generale rallenta. Ciò può verificarsi effetto collaterale indesiderato di un algoritmo di pianificazione dei thread mal progettato.</li>
      <li><strong>Deadlock</strong> : l'uso eccessivo dei blocchi mutex ha anche uno svantaggio: può entrare in deadlock nel programma. Un  deadlock è uno stato in cui un thread è in attesa che un altro thread  rilasci un blocco, ma quell'altro thread ha bisogno di una risorsa per  terminare che il primo thread sta trattenendo. In questo modo, entrambi i thread si fermano e il programma si interrompe. Deadlock può essere considerato un caso estremo di fame. Per evitare ciò, dobbiamo stare attenti a non aumentare troppi blocchi interdipendenti.</li>
      <li><strong>Livelock</strong> : Livelock è quando i thread continuano a funzionare in un ciclo ma non fanno alcun progresso. Ciò deriva anche da una progettazione scadente e dall'uso improprio dei blocchi mutex.</li>
    </ul>
    <h2 id="multiprocessingandthreadinginpython">Multiprocessing e Threading in Python</h2>
    <h3 id="theglobalinterpreterlock">Il blocco globale dell'interprete</h3>
    <p>Quando si tratta di Python, ci sono alcune stranezze da tenere a mente. Sappiamo  che i thread devono prendere lo stesso spazio di memoria, quindi è  necessario adottare speciali in modo che due thread non scrivano nella  posizione stessa di memoria. L'interprete CPython gestisce questo utilizzando un meccanismo GIL, o Global Interpreter Lock.</p>
    <p><a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://wiki.python.org/moin/GlobalInterpreterLock">Dal wiki</a> di Python :</p>
    <blockquote>
      <p>In CPython, il <strong>global interpreter lock</strong> , o <strong>GIL</strong> , è un mutex che protegge l'accesso agli oggetti Python, impedendo a  più thread di eseguire il bytecode Python contemporaneamente. Questo blocco è necessario principalmente perché la gestione della memoria di CPython non è thread-safe.</p>
    </blockquote>
    <p>Controlla le diapositive <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://www.dabeaz.com/python/UnderstandingGIL.pdf">qui</a> per uno sguardo più dettagliato a Python GIL.</p>
    <p>Il GILottenuto il suo lavoro fatto, ma ad un costo. Serializza istruzioni a livello di interpretazione. Come funziona è il seguente: affinché qualsiasi thread esegua qualsiasi funzione, deve acquisire un blocco globale. Solo un singolo thread può acquisire quel blocco alla volta, il che significa che <strong>l'interprete esegue le istruzioni in modo seriale</strong> . Questo design rende la gestione della memoria thread-safe, ma di conseguenza non può utilizzare più core della CPU. Nelle CPU single-core, che è ciò che i progettisti avevano in mente durante lo sviluppo di CPython, non è un grosso problema. Ma questo blocco globale finisce per essere un collo di bottiglia se stai usando CPU multi-core.</p>
    <p>Questo  collo di bottiglia, tuttavia, diventa irrilevante se il programma  presenta un collo di bottiglia più grave altrove, ad nella rete, nell'IO  o nell'interazione dell'utente. In questi casi, il threading è un metodo di parallelizzazione del tutto efficace. Ma per i programmi che sono vincolati alla CPU, il threading finisce per rallentare il programma. Esploriamo questo con alcuni casi d'uso di esempio.</p>
    <h3 id="usecasesforthreading">Casi d'uso per il threading</h3>
    <p>I programmi della GUI utilizzano il threading tutto il tempo per rendere le applicazioni reattive. Ad  esempio, in un programma di modifica del testo, un thread può occuparsi  della registrazione degli input dell'utente, un altro può essere  responsabile della visualizzazione del testo, un terzo può eseguire il  controllo ortografico e così via. Qui, il programma deve attendere l'interazione dell'utente, che è il collo di bottiglia più grande. L'uso del multiprocessing non renderà il programma più veloce.</p>
    <p>Un altro caso d'uso per il threading sono i programmi associati a IO o di rete, come <a href="https://blog-floydhub-com.translate.goog/web-scraping-with-python/?_x_tr_sl=auto&amp;_x_tr_tl=it&amp;_x_tr_hl=it">web-scrapers</a> . In questo caso, più thread possono occuparsi dello scraping di più pagine Web in parallelo. I  thread devono scaricare le pagine Web da Internet e questo sarà il  collo di bottiglia più grande, quindi il threading è una soluzione  perfetta qui. I legati alla rete server, essendo in modo simile; con loro, il multiprocessing non ha alcun vantaggio sul threading. Un altro esempio rilevante è <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://www.tensorflow.org/api_docs/python/tf/data/Dataset%23interleave">Tensorflow</a> , che utilizza un pool di thread per trasforma i dati in parallelo.</p>
    <h3 id="usecasesformultiprocessing">Casi d'uso per la multielaborazione</h3>
    <p>Il  multiprocessing eclissa il threading nei casi in cui il programma  richiede un uso intensivo della CPU e non deve eseguire alcuna IO o  interazione con l'utente. Ad esempio, programma che si limita a qualsiasi granocchiare i numeri vedrà un enorme aumento di velocità dal multiprocessing; infatti, il threading probabilmente lo rallenterà. Un interessante esempio del mondo reale è <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://pytorch.org/docs/stable/data.html%23torch.utils.data.DataLoader">Pytorch Dataloader</a> , che utilizza più sottoprocessi per caricare i dati nella GPU.</p>
    <h3 id="parallelizationinpythoninaction">Parallelizzazione in Python, in Action</h3>
    <p>Python offre due librerie - e - per gli omonimi metodi di parallelizzazione. Nonostante la differenza fondamentale tra loro, le due librerie offrono un'API molto simile (a partire da Python 3.7). Vediamoli in azione.<a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a><a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://docs.python.org/3/library/threading.html">threading</a></p>
    <pre>import threading import random from functools import reduce   def func(number):     random_list = random.sample(range(1000000), number)     return reduce(lambda x, y: x*y, random_list)       number = 50000 thread1 = threading.Thread(target=func, args=(number,)) thread2 = threading.Thread(target=func, args=(number,))  thread1.start() thread2.start()  thread1.join() thread2.join() </pre>
    <p>Puoi vedere che ho creato una funzione funcche crea un elenco di numeri casuali e quindi moltiplica tutti gli elementi in sequenza. Questo può essere un processo piuttosto pesante se il numero di elementi è abbastanza grande, diciamo 50k o 100k.</p>
    <p>Quindi, ho creato due thread che eseguiranno la stessa funzione. Gli oggetti thread hanno un startmetodo che avvia il thread in modo asincrono. Se vogliamo aspettare che terminino e tornino, dobbiamo chiamare il joinmetodo, ed è quello che abbiamo fatto sopra.</p>
    <p>Come puoi vedere, l'API per avviare un nuovo thread in un'attività in background è piuttosto semplice. La cosa fantastica è che anche l'API per il multiprocessing è quasi la stessa; controlliamolo.</p>
    <pre>import multiprocessing import random from functools import reduce   def func(number):     random_list = random.sample(range(1000000), number)     return reduce(lambda x, y: x*y, random_list)       number = 50000 process1 = multiprocessing.Process(target=func, args=(number,)) process2 = multiprocessing.Process(target=func, args=(number,))  process1.start() process2.start()  process1.join() process2.join() </pre>
    <p>Eccolo: basta scambiare threading.Threadcon multiprocessing.Processe hai lo stesso identico programma implementato usando il multiprocessing.</p>
    <p>Ovviamente  c'è molto di più che puoi fare con questo, ma non rientra nell'ambito  di questo articolo, quindi non ne parleremo qui. Consulta i documenti <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://docs.python.org/3/library/threading.html">qui</a> e <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://docs.python.org/3/library/threading.html">qui</a> se sei interessato a saperne di più.</p>
    <h3 id="benchmarks">Punti di riferimenti</h3>
    <p>Ora che abbiamo un'idea di come appare il codice che implementa la parallelizzazione, torniamo ai problemi di prestazioni. Come abbiamo notato prima, il threading non è adatto per attività legate alla CPU; in quei casi finisce per essere un collo di bottiglia. Possiamo convalidarlo utilizzando alcuni semplici benchmark.</p>
    <p>In primo luogo, vediamo come il threading si confronta con il multiprocessing per l'esempio di codice che ti ho mostrato sopra. Tieni presente che questa attività non coinvolge alcun tipo di IO, quindi è un'attività legata alla CPU pura.</p>
    <p><img src="https://www.elexpo.net/archivio/tps/parallelismo/Multiprocessing%20vs.%20Threading%20in%20Python%20What%20Every%20Data%20Scientist%20Needs%20to%20Know_files/Plot-1.png" alt="Trama-1" loading="lazy" /></p>
    <p>E vediamo un benchmark simile per un'attività legata all'IO. Ad esempio, la seguente funzione —</p>
    <pre>import requests  def func(number):     url = 'http://example.com/'     for i in range(number):         response = requests.get(url)         with open('example.com.txt', 'w') as output:             output.write(response.text) </pre>
    <p>La funzione è semplicemente recuperare una pagina Web e salvarla in un file locale, più volte in un ciclo. Inutile ma semplice e quindi adatto per la dimostrazione. Diamo un'occhiata al benchmark.</p>
    <p><img src="https://www.elexpo.net/archivio/tps/parallelismo/Multiprocessing%20vs.%20Threading%20in%20Python%20What%20Every%20Data%20Scientist%20Needs%20to%20Know_files/Plot-3.png" alt="Trama-3" loading="lazy" /></p>
    <p>Ora ci sono alcune cose da notare da questi due grafici:</p>
    <ul>
      <li>
        <p>In entrambi i casi, un singolo processo ha richiesto più tempo di esecuzione di un singolo thread. Evidentemente, i processi hanno un sovraccarico maggiore dei thread.</p>
      </li>
      <li>
        <p>Per l'attività legata alla CPU, più processi funzionano molto meglio di più thread. Tuttavia, questa differenza diventa leggermente meno evidente quando utilizziamo la parallelizzazione 8x. Poiché il processore del mio laptop è quad-core, fino a quattro processi possono utilizzare più core in modo efficace. Quindi, quando utilizzo più processi, non si ridimensiona molto bene. Tuttavia, supera di gran lunga il threading perché il threading non può utilizzare affatto i core multipli.</p>
      </li>
      <li>
        <p>Per l'attività legata all'IO, il collo di bottiglia non è la CPU. Quindi le solite limitazioni dovute a GIL non si applicano qui e il multiprocessing non ha alcun vantaggio. Non  solo, il leggero sovraccarico dei thread li rende effettivamente più  veloci del multiprocessing e il threading finisce per superare  costantemente il multiprocessing.</p>
      </li>
    </ul>
    <h1 id="differencesmeritsanddrawbacks">Differenze, pregi e svantaggi</h1>
    <ul>
      <li>
        <p>I thread vengono eseguiti nello stesso spazio di memoria; i processi hanno una memoria separata.</p>
      </li>
      <li>
        <p>Seguendo  il punto precedente: condividere oggetti tra thread è più semplice, ma  il rovescio della medaglia è che devi prendere misure extra per la  sincronizzazione degli oggetti per assicurarti che due thread non  scrivano sullo stesso oggetto contemporaneamente e che una condizione di  razza non si verifica.</p>
      </li>
      <li>
        <p>A  causa del sovraccarico di programmazione aggiuntivo della  sincronizzazione degli oggetti, la programmazione multithread è più  soggetta a bug. D'altra parte, la programmazione multiprocesso è facile da ottenere.</p>
      </li>
      <li>
        <p>I thread hanno un sovraccarico inferiore rispetto ai processi; i processi di spawn richiedono più tempo dei thread.</p>
      </li>
      <li>
        <p>A causa delle limitazioni messe in atto dal GIL in Python, i thread non possono ottenere un <em>vero</em> parallelismo utilizzando più core della CPU. Il multiprocessing non ha tali restrizioni.</p>
      </li>
      <li>
        <p><a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://en.wikipedia.org/wiki/Scheduling_(computing)">La pianificazione dei</a> processi è gestita dal sistema operativo, mentre la pianificazione dei thread viene eseguita dall'interprete Python.</p>
      </li>
      <li>
        <p>I processi figlio sono interrompibili e killable, mentre i thread figlio non lo sono. Devi aspettare che i thread finiscano o join.</p>
      </li>
    </ul>
    <p>Da tutta questa discussione, possiamo concludere quanto segue:</p>
    <ul>
      <li>
        <p>Il threading dovrebbe essere utilizzato per i programmi che coinvolgono l'IO o l'interazione dell'utente.</p>
      </li>
      <li>
        <p>Il multiprocessing dovrebbe essere utilizzato per i programmi ad alta intensità di calcolo legati alla CPU.</p>
      </li>
    </ul>
    <h1 id="fromtheperspectiveofadatascientist">Dal punto di vista di un data scientist</h1>
    <p>Una tipica pipeline di elaborazione dati può essere suddivisa nei seguenti passaggi:</p>
    <ol>
      <li>Lettura di dati grezzi e archiviazione nella memoria principale o nella GPU</li>
      <li>Esecuzione di calcoli, utilizzando CPU o GPU</li>
      <li>Memorizzazione delle informazioni estratte in un database o disco.</li>
    </ol>
    <p>Esaminiamo come introdurre il parallelismo in queste attività in modo che possano essere velocizzate.</p>
    <p>Il  passaggio 1 prevede la lettura dei dati dal disco, quindi chiaramente  l'IO del disco sarà il collo di bottiglia per questo passaggio. Come abbiamo discusso, i thread sono l'opzione migliore per parallelizzare questo tipo di operazioni. Allo stesso modo, anche il passaggio 3 è un candidato ideale per l'introduzione della filettatura.</p>
    <p>Tuttavia, il passaggio 2 consiste in calcoli che coinvolgono la CPU o una GPU. Se si tratta di un'attività basata sulla CPU, l'uso del threading non sarà di alcuna utilità; invece, dobbiamo optare per il multiprocessing. Solo così potremo sfruttare i molteplici core della CPU e ottenere il parallelismo. Se  si tratta di un'attività basata su GPU, poiché la GPU implementa già  un'architettura massicciamente parallelizzata a livello hardware,  l'utilizzo dell'interfaccia corretta (librerie e driver) per interagire  con la GPU dovrebbe occuparsi del resto.</p>
    <p><img src="https://www.elexpo.net/archivio/tps/parallelismo/Multiprocessing%20vs.%20Threading%20in%20Python%20What%20Every%20Data%20Scientist%20Needs%20to%20Know_files/mutli-t-nd-mutli-p.png" alt="mutli-t-nd-mutli-p" loading="lazy" /></p>
    <p>Ora potresti pensare: &quot;La mia pipeline di dati sembra leggermente diversa da questa; Ho alcuni compiti che non rientrano in questo quadro generale&quot;. Tuttavia, dovresti essere in grado di osservare la metodologia utilizzata qui per decidere tra threading e multiprocessing. I fattori che dovresti considerare sono:</p>
    <ul>
      <li>Se la tua attività ha una qualsiasi forma di IO</li>
      <li>Se IO è il collo di bottiglia del tuo programma</li>
      <li>Se la tua attività dipende da una grande quantità di calcolo da parte della CPU</li>
    </ul>
    <p>Con questi fattori in mente, insieme ai suggerimenti di cui sopra, dovresti essere in grado di prendere la decisione. Inoltre, tieni presente che non è necessario utilizzare una singola forma di parallelismo in tutto il programma. Dovresti usare l'uno o l'altro per diverse parti del tuo programma, a seconda di quella adatta per quella particolare parte.</p>
    <p>Ora  esamineremo due scenari di esempio che un data scientist potrebbe dover  affrontare e come puoi utilizzare il calcolo parallelo per  velocizzarli.</p>
    <h2 id="scenariodownloadingemails">Scenario: download di e-mail</h2>
    <p>Diciamo  che vuoi analizzare tutte le email presenti nella posta in arrivo della  tua startup autoctona e capirne le tendenze: chi sono i mittenti più  frequenti, quali sono le parole chiave più comuni che compaiono nelle  email, quale giorno della settimana o quale ora del giorno ricevi la  maggior parte delle email e così via. Il primo passo di questo progetto sarebbe, ovviamente, scaricare le email sul tuo computer.</p>
    <p>All'inizio, eseguiamolo in sequenza senza utilizzare alcuna parallelizzazione. Il codice da usare è di seguito e dovrebbe essere abbastanza autoesplicativo. C'è una funzione download_emailsche prende un elenco di ID e-mail come input e li scarica in sequenza. Questo chiama questa funzione con un elenco di ID di 100 e-mail contemporaneamente.</p>
    <pre>import imaplib import time  IMAP_SERVER = 'imap.gmail.com' USERNAME = 'username@gmail.com' PASSWORD = 'password'  def download_emails(ids):     client = imaplib.IMAP4_SSL(IMAP_SERVER)     client.login(USERNAME, PASSWORD)     client.select()     for i in ids:         print(f'Downloading mail id: {i.decode()}')         _, data = client.fetch(i, '(RFC822)')         with open(f'emails/{i.decode()}.eml', 'wb') as f:             f.write(data[0][1])     client.close()     print(f'Downloaded {len(ids)} mails!')      start = time.time()  client = imaplib.IMAP4_SSL(IMAP_SERVER) client.login(USERNAME, PASSWORD) client.select() _, ids = client.search(None, 'ALL') ids = ids[0].split() ids = ids[:100] client.close()  download_emails(ids) print('Time:', time.time() - start) </pre>
    <p>Tempo impiegato :: 35.65300488471985secondi.</p>
    <p>Ora introduciamo un po' di parallelizzabilità in questa attività per velocizzare le cose. Prima di immergerci nella scrittura del codice, dobbiamo decidere tra threading e multiprocessing. Come  hai appreso finora, i thread sono l'opzione migliore quando si tratta  di attività che hanno alcuni IO come collo di bottiglia. Il  compito in questione appartiene ovviamente a questa categoria, in  quanto si tratta di accedere a un server IMAP tramite Internet. Quindi andremo con  threading.</p>
    <p>Gran parte del codice che useremo sarà lo stesso che abbiamo usato nel caso sequenziale. L'unica  differenza è che divideremo l'elenco di 100 ID e-mail in 10 blocchi più  piccoli, ciascuno contenente 10 ID, quindi creeremo 10 thread e  chiameremo la download_emailsfunzione con un blocco diverso da ciascuno di essi. Sto usando la concurrent.futures.ThreadPoolExecutorclasse dalla libreria standard Python per il threading.</p>
    <pre>import imaplib import time from concurrent.futures import ThreadPoolExecutor  IMAP_SERVER = 'imap.gmail.com' USERNAME = 'username@gmail.com' PASSWORD = 'password'  def download_emails(ids):     client = imaplib.IMAP4_SSL(IMAP_SERVER)     client.login(USERNAME, PASSWORD)     client.select()     for i in ids:         print(f'Downloading mail id: {i.decode()}')         _, data = client.fetch(i, '(RFC822)')         with open(f'emails/{i.decode()}.eml', 'wb') as f:             f.write(data[0][1])     client.close()     print(f'Downloaded {len(ids)} mails!')      start = time.time()  client = imaplib.IMAP4_SSL(IMAP_SERVER) client.login(USERNAME, PASSWORD) client.select() _, ids = client.search(None, 'ALL') ids = ids[0].split() ids = ids[:100] client.close()  number_of_chunks = 10 chunk_size = 10 executor = ThreadPoolExecutor(max_workers=number_of_chunks) futures = [] for i in range(number_of_chunks):     chunk = ids[i*chunk_size:(i+1)*chunk_size]     futures.append(executor.submit(download_emails, chunk))  for future in concurrent.futures.as_completed(futures):     pass print('Time:', time.time() - start) </pre>
    <p>Tempo impiegato :: 9.841094255447388secondi.</p>
    <p>Come puoi vedere, il threading lo ha accelerato notevolmente.</p>
    <h2 id="scenarioclassificationusingscikitlearn">Scenario: classificazione utilizzando Scikit-Learn</h2>
    <p>Supponiamo che tu abbia un problema di classificazione e desideri utilizzare un classificatore di <em>foresta casuale</em> per questo. Poiché si tratta di un algoritmo di apprendimento automatico standard e ben noto, non <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://en.wikipedia.org/wiki/Reinventing_the_wheel">reinventiamo la ruota</a> e utilizziamo solo sklearn.ensemble.RandomForestClassifier.</p>
    <p>Il codice seguente serve a scopo dimostrativo. Ho creato un set di dati di classificazione utilizzando la funzione helper sklearn.datasets.make_classification, quindi ho eseguito il training RandomForestClassifiersu quello. Inoltre, sto cronometrando la parte del codice che svolge il lavoro principale di adattamento del modello.</p>
    <pre>from sklearn.ensemble import RandomForestClassifier from sklearn import datasets import time  X, y = datasets.make_classification(n_samples=10000, n_features=50, n_informative=20, n_classes=10)  start = time.time() model = RandomForestClassifier(n_estimators=500) model.fit(X, y) print('Time:', time.time()-start)  </pre>
    <p>Tempo impiegato :: 34.17733192443848secondi.</p>
    <p>Ora esamineremo come possiamo ridurre il tempo di esecuzione di questo algoritmo. Sappiamo che questo algoritmo può essere parallelizzato in una certa misura, ma che tipo di parallelizzazione sarebbe adatta? Non ha alcun collo di bottiglia IO; al contrario, è un'attività molto impegnativa per la CPU. Quindi il multiprocessing sarebbe la scelta logica.</p>
    <p>Fortunatamente, sklearnha già implementato il multiprocessing in questo algoritmo e non dovremo scriverlo da zero. Come puoi vedere nel codice seguente, dobbiamo solo fornire un parametro n_jobs, il numero di processi che dovrebbe utilizzare, per abilitare il multiprocessing.</p>
    <pre>from sklearn.ensemble import RandomForestClassifier from sklearn import datasets import time  X, y = datasets.make_classification(n_samples=10000, n_features=50, n_informative=20, n_classes=10)  start = time.time() model = RandomForestClassifier(n_estimators=500, n_jobs=4) model.fit(X, y) print('Time:', time.time()-start) </pre>
    <p>Tempo impiegato :: 14.576200723648071secondi.</p>
    <p>Come previsto, il multiprocessing lo ha reso un po' più veloce.</p>
    <h1 id="conclusion">Conclusione</h1>
    <p>La maggior parte, se non tutti i progetti di scienza dei dati vedranno un enorme aumento di velocità con il calcolo parallelo. In effetti, molte delle popolari librerie di data science hanno già il parallelismo integrato, <em>devi solo abilitarlo</em> . Quindi  prima di provare a implementarlo da solo, sfoglia la documentazione  della libreria che stai utilizzando e controlla se supporta il  parallelismo (a proposito, ti consiglio assolutamente di dare  un'occhiata a <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://dask.org/">dask</a> ). In caso contrario, questo articolo dovrebbe aiutarti a implementarlo da solo.    </p>
    <h1 id="abouttheauthor">Circa l'autore</h1>
    <p>Sumit è un appassionato di computer che ha iniziato a programmare in tenera età; attualmente sta terminando il suo master in informatica presso l'IIT Delhi. Ogni  volta che non sta programmando, probabilmente puoi trovarlo a leggere  filosofia, suonare la chitarra, scattare foto o bloggare. Puoi connetterti con Sumit su <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://twitter.com/SkullTech101">Twitter</a> , <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://www.linkedin.com/in/sumit-ghosh101">LinkedIn</a> , <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://github.com/SkullTech">Github</a> e <a href="https://translate.google.com/website?sl=auto&amp;tl=it&amp;u=https://sumit-ghosh.com/">il suo sito web</a> .</p>
  </div>
</div>
</body>
</html>
